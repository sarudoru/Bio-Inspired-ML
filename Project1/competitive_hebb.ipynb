{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e21d23",
   "metadata": {},
   "source": [
    "**Jacob Petty, Sardor Nodirov, and Saad Khan**\n",
    "\n",
    "Spring 2026\n",
    "\n",
    "CS 443: Bio-inspired Machine Learning\n",
    "\n",
    "Project 1: Hebbian Learning\n",
    "\n",
    "#### Week 2: Competitive Hebbian Network\n",
    "\n",
    "This week you will implement a bio-inspired network that learns according to a competitive variant of Oja's Rule proposed by Krotov & Hopfield (2019). You will train this network on MNIST and CIFAR-10 while dynamically visualizing the learning process!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f98e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from image_datasets import get_dataset, train_val_split\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f660a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print('Running on the GPU')\n",
    "else:\n",
    "    print('NOT running on the GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d6852",
   "metadata": {},
   "source": [
    "## Task 5: Building the Competitive Hebbian Network\n",
    "\n",
    "Implement the Hebbian learning network in `hebb_net.py` and test it using the cells below.\n",
    "\n",
    "As with your `LinearDecoder`, your Competitive Hebbian network should be implemented entirely using the low-level TensorFlow API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3025e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hebb_net import HebbNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33daa10",
   "metadata": {},
   "source": [
    "### 5a. Start implementing Hebbian network (`HebbNet` class)\n",
    "\n",
    "Implement the following methods in `hebb_net.py`:\n",
    "- Constructor\n",
    "- `get_wts(self)`\n",
    "- `set_wts(self, wts)`\n",
    "- `net_in(self, x)`\n",
    "\n",
    "The network uses the usual Dense net in:\n",
    "\n",
    "$$\\text{netIn}_{ih} = z_{ih} = \\sum_{j=1}^M x_{ij} w_{jh}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7f773",
   "metadata": {},
   "source": [
    "#### Test: `net_in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized wts randomly.\n",
      "Your net_in is:\n",
      "[[ 1.232 -0.955  2.153  2.175 -3.539 -0.031]\n",
      " [ 1.957  0.084  1.141 -0.243 -3.882 -1.126]\n",
      " [ 2.25  -0.947  0.124 -0.15  -5.327 -1.171]]\n",
      "and it should be:\n",
      "[[ 1.232 -0.955  2.153  2.175 -3.539 -0.031]\n",
      " [ 1.957  0.084  1.141 -0.243 -3.882 -1.126]\n",
      " [ 2.25  -0.947  0.124 -0.15  -5.327 -1.171]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771449848.599038  489152 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1771449848.599085  489152 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "N, M, H = 3, 10, 6\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.uniform(shape=(N, M), seed=0)\n",
    "\n",
    "net = HebbNet(M, H, k=3)\n",
    "print(f'Your net_in is:\\n{net.net_in(x)}\\nand it should be:')\n",
    "print('''[[ 1.232 -0.955  2.153  2.175 -3.539 -0.031]\n",
    " [ 1.957  0.084  1.141 -0.243 -3.882 -1.126]\n",
    " [ 2.25  -0.947  0.124 -0.15  -5.327 -1.171]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c11f9",
   "metadata": {},
   "source": [
    "### 5b. Preparation for implementing Hebbian activation\n",
    "\n",
    "This is a multi-part task is geared toward discovering and practicing TensorFlow operations that will guide you through the implementation of the Hebbian network activation function.\n",
    "\n",
    "#### (i) Find the index of the top-k items in each row of a Tensor\n",
    "\n",
    "Here is a mock scenario for this task (not specifically related to `HebbNet`). We want to find the index of the 2nd largest item in each row of a tensor. While in this case we are interested in the `k=2` largest, your solution should in theory work for any `k`. This is like `argmax` within each row, but more configurable. We may want the index of the max (`k=1`), the 2nd largest (`k=2`), 3rd largest (`k=3`), and so forth. If there are ties for `k`-th place, the smallest column index is fine.\n",
    "\n",
    "Ultimately, we want the column indices to have a `tf.int32` data type.\n",
    "\n",
    "**Hint:** There is a hint in the title of this subtask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc0e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_tensor is:\n",
      "[[ 1.5  4.6  2.2  3.5]\n",
      " [ 9.9 10.   5.6  1.6]\n",
      " [ 9.9  6.8  9.4  7.6]\n",
      " [ 8.6  4.3  3.9  7. ]\n",
      " [ 6.5  0.8  3.7  0.6]\n",
      " [ 9.7  1.   4.   1.7]\n",
      " [ 8.5  6.4  9.   7.1]]\n",
      "Column indices shape is (7,) and should be (7,).\n",
      "Column indices of the 2nd largest values in each row are:\n",
      "[3 0 2 3 2 2 0]\n",
      "They should be\n",
      "[3 0 2 3 2 2 0]\n",
      "Dtype is <dtype: 'int32'> as it should be <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2)\n",
    "my_tensor = tf.cast(tf.math.round(10*tf.random.uniform(maxval=10, shape=(7, 4)))/10, dtype=tf.float32)\n",
    "\n",
    "col_indices = tf.math.top_k(my_tensor, k=2).indices[:, 1]\n",
    "\n",
    "print(f'my_tensor is:\\n{my_tensor}')\n",
    "print(f'Column indices shape is {col_indices.shape} and should be (7,).')\n",
    "print(f'Column indices of the 2nd largest values in each row are:\\n{col_indices}')\n",
    "print('They should be\\n[3 0 2 3 2 2 0]')\n",
    "print(f\"Dtype is {col_indices.dtype} as it should be <dtype: 'int32'>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681ab4e",
   "metadata": {},
   "source": [
    "#### (ii) Generating sequences of values\n",
    "\n",
    "Generate a 1D Tensor of `tf.int32` values that go from 0 to the number of rows in `my_tensor` - 1, in steps of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0d437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row indices shape is (7,) and should be (7,).\n",
      "Your row indices are:\n",
      "[0 1 2 3 4 5 6]\n",
      "They should be\n",
      "[0 1 2 3 4 5 6]\n",
      "Dtype is <dtype: 'int32'> as it should be <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "row_indices = tf.range(tf.shape(my_tensor)[0], dtype=tf.int32)\n",
    "print(f'Row indices shape is {row_indices.shape} and should be (7,).')\n",
    "print(f'Your row indices are:\\n{row_indices}')\n",
    "print('They should be\\n[0 1 2 3 4 5 6]')\n",
    "print(f\"Dtype is {row_indices.dtype} as it should be <dtype: 'int32'>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84a9a8",
   "metadata": {},
   "source": [
    "#### (iii) Stacking multiple Tensors together\n",
    "\n",
    "Now you have a list of row indices (`row_indices`) and a list of column indices (`col_indices`). Write code to stack them into a single Tensor where the first column holds the row indices and the second column holds the column indices. In other words, we are creating row-col index pairs in each row.\n",
    "\n",
    "Example:\n",
    "`rows: [0, 1, 2]`\n",
    "`cols: [9, 8, 7]`\n",
    "\n",
    "We want combined:\n",
    "```\n",
    "0, 9\n",
    "1, 8\n",
    "2, 7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b6e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows are: [0 1 2 3 4 5 6]\n",
      "Cols are: [3 0 2 3 2 2 0]\n",
      "Your combined Tensor is:\n",
      "[[0 3]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [4 2]\n",
      " [5 2]\n",
      " [6 0]] and it should be:\n",
      "[[0 3]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [4 2]\n",
      " [5 2]\n",
      " [6 0]]\n"
     ]
    }
   ],
   "source": [
    "rc_indices = tf.stack([row_indices, col_indices], axis=1)\n",
    "print(f'Rows are: {row_indices}')\n",
    "print(f'Cols are: {col_indices}')\n",
    "print(f'Your combined Tensor is:\\n{rc_indices} and it should be:')\n",
    "print('''[[0 3]\n",
    " [1 0]\n",
    " [2 2]\n",
    " [3 3]\n",
    " [4 2]\n",
    " [5 2]\n",
    " [6 0]]''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dedd6a4",
   "metadata": {},
   "source": [
    "#### (iv) Creating a mask in TensorFlow\n",
    "\n",
    "Let's say we want to create a Tensor of all 0s, except have some constant value at the locations of the 2nd largest items you found above in `my_tensor`. This Tensor should have the same shape as `my_tensor`. Let's make that constant value `99` here. Figure out how to use the TensorFlow function [scatter_nd](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) to accomplish this.\n",
    "\n",
    "*Creating a Tensor that is the same shape as the original Tensor with all 0s except at preset locations is called a **mask**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75659ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated tensor is:\n",
      "[[ 0  0  0 99]\n",
      " [99  0  0  0]\n",
      " [ 0  0 99  0]\n",
      " [ 0  0  0 99]\n",
      " [ 0  0 99  0]\n",
      " [ 0  0 99  0]\n",
      " [99  0  0  0]] and should be:\n",
      "[[ 0  0  0 99]\n",
      " [99  0  0  0]\n",
      " [ 0  0 99  0]\n",
      " [ 0  0  0 99]\n",
      " [ 0  0 99  0]\n",
      " [ 0  0 99  0]\n",
      " [99  0  0  0]]\n",
      "Column indices of the 2nd largest values in each row are:\n",
      "[3 0 2 3 2 2 0]\n",
      "They should be\n",
      "[3 0 2 3 2 2 0]\n",
      "Dtype is <dtype: 'int32'> as it should be <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "mask_tensor = tf.scatter_nd(rc_indices, tf.fill(tf.shape(rc_indices)[0], 99), tf.shape(my_tensor))\n",
    "\n",
    "print(f'updated tensor is:\\n{mask_tensor} and should be:')\n",
    "print('''[[ 0  0  0 99]\n",
    " [99  0  0  0]\n",
    " [ 0  0 99  0]\n",
    " [ 0  0  0 99]\n",
    " [ 0  0 99  0]\n",
    " [ 0  0 99  0]\n",
    " [99  0  0  0]]''')\n",
    "print(f'Column indices of the 2nd largest values in each row are:\\n{tf.squeeze(col_indices)}')\n",
    "print('They should be\\n[3 0 2 3 2 2 0]')\n",
    "print(f\"Dtype is {col_indices.dtype} as it should be <dtype: 'int32'>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b048fc",
   "metadata": {},
   "source": [
    "### 5c. Hebbian network activation\n",
    "\n",
    "Now you are ready to implement the Hebbian network activation (`net_act` method)!\n",
    "\n",
    "The activation in the Hebbian network (`net_act`) to each sample $i$ in neuron $h$ implements a **competitive** process:\n",
    "\n",
    "$$\\text{netAct}_{ih} = f(z_{ih})  =\n",
    "\\begin{cases}\n",
    "      1, & \\text{if}\\ h = argmax_h (\\text{netIn}_{ih}) \\text{ i.e neuron } h \\text{ comes in 1st place} \\\\\n",
    "      -\\Delta, & \\text{if neuron }\\ h \\text{ comes in } k^{\\text{th}} \\text{ place} \\\\\n",
    "      0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Notes about the above netAct equation:\n",
    "\n",
    "- \"1st place\" refer to the neuron that achieves the highest netIn values.\n",
    "- \"$k^{\\text{th}}$ place\" refers to the neuron that achieves the $k^{\\text{th}}$ highest netIn value. For example, $k=2$ refers to the \"2nd place\" neuron that achieves the 2nd highest netIn.\n",
    "- $k^{\\text{th}}$ place is defined in the usual \"human-interpretable\" rather than an \"Python indexing\" sense (i.e. $k=2$ for second place, $k=3$ for third place, etc.)\n",
    "\n",
    "Implement the neural competition in the `net_act(self, net_in)` method then test it below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69925e",
   "metadata": {},
   "source": [
    "#### Test: net_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c6eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net_act(x, M, H, kth_place_inhibited, inhib_value):\n",
    "    '''net_act tester function'''\n",
    "    tf.random.set_seed(9)\n",
    "    net = HebbNet(M, H, k=kth_place_inhibited, inhib_value=inhib_value)\n",
    "    net_in = net.net_in(x)\n",
    "    print(f\"The net_in is:\\n{net_in}:\")\n",
    "    print(f'So your net_act is:\\n{net.net_act(net_in)}\\nand it should be:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91fc5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Simplest test with 1 sample and no inhibition (only winner-take-all)\n",
      "Initialized wts randomly.\n",
      "The net_in is:\n",
      "[[-1.464  1.609  0.766 -0.849  0.427]]:\n",
      "So your net_act is:\n",
      "[[0. 1. 0. 0. 0.]]\n",
      "and it should be:\n",
      "[[0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Simplest test\n",
    "print('Test 1: Simplest test with 1 sample and no inhibition (only winner-take-all)')\n",
    "x = np.array([[1, 2, 0]])\n",
    "test_net_act(x, M=3, H=5, kth_place_inhibited=2, inhib_value=0)\n",
    "print('[[0. 1. 0. 0. 0.]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "838c2f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Test 2: 3 samples and no inhibition (only winner-take-all)\n",
      "Initialized wts randomly.\n",
      "The net_in is:\n",
      "[[  0.801   0.835   1.712   1.813  -0.401]\n",
      " [ -0.774   0.094   1.719   1.048   1.03 ]\n",
      " [-12.44    1.404   8.395  -0.957  10.377]]:\n",
      "So your net_act is:\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "and it should be:\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "----------------------------------------------------------------------\n",
      "Test 3: This should still work — 3 samples and no inhibition targeting winner (still only winner-take-all)\n",
      "Initialized wts randomly.\n",
      "The net_in is:\n",
      "[[  0.801   0.835   1.712   1.813  -0.401]\n",
      " [ -0.774   0.094   1.719   1.048   1.03 ]\n",
      " [-12.44    1.404   8.395  -0.957  10.377]]:\n",
      "So your net_act is:\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "and it should be:\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Test with additional samples\n",
    "print(70*'-')\n",
    "print('Test 2: 3 samples and no inhibition (only winner-take-all)')\n",
    "x = np.array([[0, 1, 2], [1, 0, 1], [10, 1, 0]])\n",
    "test_net_act(x, M=3, H=5, kth_place_inhibited=2, inhib_value=0)\n",
    "print('''[[0. 0. 0. 1. 0.]\n",
    " [0. 0. 1. 0. 0.]\n",
    " [0. 0. 0. 0. 1.]]''')\n",
    "\n",
    "print(70*'-')\n",
    "print('Test 3: This should still work — 3 samples and no inhibition targeting winner (still only winner-take-all)')\n",
    "x = np.array([[0, 1, 2], [1, 0, 1], [10, 1, 0]])\n",
    "test_net_act(x, M=3, H=5, kth_place_inhibited=1, inhib_value=0)\n",
    "print('''[[0. 0. 0. 1. 0.]\n",
    " [0. 0. 1. 0. 0.]\n",
    " [0. 0. 0. 0. 1.]]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2f48e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4: Test with 1 sample, 2nd place inhibited\n",
      "Initialized wts randomly.\n",
      "The net_in is:\n",
      "[[-1.464  1.609  0.766 -0.849  0.427]]:\n",
      "So your net_act is:\n",
      "[[ 0.   1.  -0.5  0.   0. ]]\n",
      "and it should be:\n",
      "[[ 0.   1.  -0.5  0.   0. ]]\n",
      "----------------------------------------------------------------------\n",
      "Test 5: 3 samples, 2nd place inhibited\n",
      "Initialized wts randomly.\n",
      "The net_in is:\n",
      "[[  0.801   0.835   1.712   1.813  -0.401]\n",
      " [ -0.774   0.094   1.719   1.048   1.03 ]\n",
      " [-12.44    1.404   8.395  -0.957  10.377]]:\n",
      "So your net_act is:\n",
      "[[ 0.   0.  -0.5  1.   0. ]\n",
      " [ 0.   0.   1.  -0.5  0. ]\n",
      " [ 0.   0.  -0.5  0.   1. ]]\n",
      "and it should be:\n",
      "[[ 0.   0.  -0.5  1.   0. ]\n",
      " [ 0.   0.   1.  -0.5  0. ]\n",
      " [ 0.   0.  -0.5  0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Tests with 2nd place inhibited\n",
    "print('Test 4: Test with 1 sample, 2nd place inhibited')\n",
    "x = np.array([[1, 2, 0]])\n",
    "test_net_act(x, M=3, H=5, kth_place_inhibited=2, inhib_value=-0.5)\n",
    "print('[[ 0.   1.  -0.5  0.   0. ]]')\n",
    "\n",
    "print(70*'-')\n",
    "print('Test 5: 3 samples, 2nd place inhibited')\n",
    "x = np.array([[0, 1, 2], [1, 0, 1], [10, 1, 0]])\n",
    "test_net_act(x, M=3, H=5, kth_place_inhibited=2, inhib_value=-0.5)\n",
    "print('''[[ 0.   0.  -0.5  1.   0. ]\n",
    " [ 0.   0.   1.  -0.5  0. ]\n",
    " [ 0.   0.  -0.5  0.   1. ]]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6daaccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6: Test with 3 samples, 4th place inhibited\n",
      "Initialized wts randomly.\n",
      "The net_in is:\n",
      "[[  0.801   0.835   1.712   1.813  -0.401]\n",
      " [ -0.774   0.094   1.719   1.048   1.03 ]\n",
      " [-12.44    1.404   8.395  -0.957  10.377]]:\n",
      "So your net_act is:\n",
      "[[-0.25  0.    0.    1.    0.  ]\n",
      " [ 0.   -0.25  1.    0.    0.  ]\n",
      " [ 0.    0.    0.   -0.25  1.  ]]\n",
      "and it should be:\n",
      "[[-0.25  0.    0.    1.    0.  ]\n",
      " [ 0.   -0.25  1.    0.    0.  ]\n",
      " [ 0.    0.    0.   -0.25  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Tests with 4th place inhibited\n",
    "print('Test 6: Test with 3 samples, 4th place inhibited')\n",
    "x = np.array([[0, 1, 2], [1, 0, 1], [10, 1, 0]])\n",
    "test_net_act(x, M=3, H=5, kth_place_inhibited=4, inhib_value=-0.25)\n",
    "print('''[[-0.25  0.    0.    1.    0.  ]\n",
    " [ 0.   -0.25  1.    0.    0.  ]\n",
    " [ 0.    0.    0.   -0.25  1.  ]]''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f19d6368",
   "metadata": {},
   "source": [
    "### 5d. Hebbian weight update\n",
    "\n",
    "The competitive Hebbian network uses the Competitive Oja's Rule for weight updates. The first part is computing the weight change:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{w}_{jh} = \\sum_{i=1}^Bx_{ij}\\text{netAct}_{ih} - w_{jh}\\sum_{i=1}^B\\text{netIn}_{ih}\\text{netAct}_{ih}\n",
    "$$\n",
    "\n",
    "The second part is actually updating the weights with the weight change:\n",
    "$$\n",
    "w_{jh}(t) = w_{jh}(t-1) + \\eta \\frac{\\Delta \\text{w}_{jh}}{\\max{|\\text{w}|} + \\epsilon} \n",
    "$$\n",
    "\n",
    "where $\\max{| \\text{w}|}$ is the absolute value of the maximum of all the `(M, H)` current weights (a float) and $\\epsilon$ is a small positive number to prevent possible division by 0.\n",
    "\n",
    "Implement the neural competition in the `update_wts(self, x, net_in, net_act, lr, eps=1e-10)` method then test it below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20609e86",
   "metadata": {},
   "source": [
    "#### Test: `update_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f18d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized wts randomly.\n",
      "Your wts after 1 update are:\n",
      "[[ 0.928 -0.059 -0.531  0.092 -0.178 -0.919]\n",
      " [-2.052 -2.894  0.803 -0.931 -0.372 -0.321]\n",
      " [-2.775 -2.993 -0.524  2.044 -1.503 -0.921]\n",
      " [ 1.462 -1.516 -0.133  0.09  -1.964 -1.298]\n",
      " [-0.232 -1.578 -0.48   0.049  0.725 -0.216]\n",
      " [-1.219  1.166 -1.384 -0.897 -0.05   0.696]\n",
      " [ 0.028 -2.214  0.679  0.963 -0.197  0.157]\n",
      " [-0.031  1.126 -1.05  -0.434 -1.022 -0.165]\n",
      " [ 2.195 -0.087  0.121  0.095 -0.482  0.056]\n",
      " [ 0.078 -2.283  0.027 -0.676  0.146 -0.822]]\n",
      "and it should be:\n",
      "[[ 0.928 -0.059 -0.531  0.092 -0.178 -0.919]\n",
      " [-2.052 -2.894  0.803 -0.931 -0.372 -0.321]\n",
      " [-2.775 -2.993 -0.524  2.044 -1.503 -0.921]\n",
      " [ 1.462 -1.516 -0.133  0.09  -1.964 -1.298]\n",
      " [-0.232 -1.578 -0.48   0.049  0.725 -0.216]\n",
      " [-1.219  1.166 -1.384 -0.897 -0.05   0.696]\n",
      " [ 0.028 -2.214  0.679  0.963 -0.197  0.157]\n",
      " [-0.031  1.126 -1.05  -0.434 -1.022 -0.165]\n",
      " [ 2.195 -0.087  0.121  0.095 -0.482  0.056]\n",
      " [ 0.078 -2.283  0.027 -0.676  0.146 -0.822]]\n"
     ]
    }
   ],
   "source": [
    "N, M, H = 3, 10, 6\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.uniform(shape=(N, M))\n",
    "\n",
    "net = HebbNet(M, H, k=3)\n",
    "net_in = net.net_in(x)\n",
    "net_act = net.net_act(net_in)\n",
    "net.update_wts(x, net_in, net_act, lr=0.5)\n",
    "print(f'Your wts after 1 update are:\\n{net.get_wts()}\\nand it should be:')\n",
    "print('''[[ 0.928 -0.059 -0.531  0.092 -0.178 -0.919]\n",
    " [-2.052 -2.894  0.803 -0.931 -0.372 -0.321]\n",
    " [-2.775 -2.993 -0.524  2.044 -1.503 -0.921]\n",
    " [ 1.462 -1.516 -0.133  0.09  -1.964 -1.298]\n",
    " [-0.232 -1.578 -0.48   0.049  0.725 -0.216]\n",
    " [-1.219  1.166 -1.384 -0.897 -0.05   0.696]\n",
    " [ 0.028 -2.214  0.679  0.963 -0.197  0.157]\n",
    " [-0.031  1.126 -1.05  -0.434 -1.022 -0.165]\n",
    " [ 2.195 -0.087  0.121  0.095 -0.482  0.056]\n",
    " [ 0.078 -2.283  0.027 -0.676  0.146 -0.822]]''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57bdc189",
   "metadata": {},
   "source": [
    "## Task 6: Train the Competitive Hebbian Network\n",
    "\n",
    "### 6a. Implement `fit` and train Hebbian network on MNIST\n",
    "\n",
    "Now is the time to train your Hebbian network on the MNIST training set! Implement the `fit` method and train your Hebbian network with the below hyperparameters. Given the high computational demands, you will want to train your Hebbian network using a GPU.\n",
    "\n",
    "#### Goal\n",
    "\n",
    "The goal is to train the Hebbian network so that the netAct values provide an accurate encoding of the MNIST digits. Once the network weights enable this (after the training that you do in this task), next week you will train your neural networks to decode the class label from the Hebbian network netAct values.\n",
    "\n",
    "#### Live plotting/animation\n",
    "\n",
    "You will want to set `plot_wts_live`: set to True so that you can visualize the weights dynamically during training every `print_every` epochs! It's really cool to see the learning process in action and it's also a great debugging tool! \n",
    "\n",
    "You will want to get this working — it is worth it!\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "- Train on the Hebbian network with the full, **centered** 60,000 samples from the training set. Do not create a validation set here. *Since we create the validation set from the training set, training the Hebbian network on the full 60k training set samples will set us up for training the decoder networks with a validation set next week.*\n",
    "- 500 epochs\n",
    "- 2000 neurons\n",
    "- 512 sample mini-batch size\n",
    "- $k = 6$ neurons that achieve the 6th highest netIn should be inhibited to $-\\Delta$\n",
    "- $\\Delta = 0.4$\n",
    "- `5e-3` learning rate\n",
    "\n",
    "#### Tips\n",
    "- If your dynamic weight plots (after epoch 0) look mostly white with some red streaks, try transposing your weight matrix before passing it into `draw_grid_image`!\n",
    "- Make sure you pass in the correct feature shape into `fit`.\n",
    "- Your weights should range from (roughly) (-0.5, +0.5) by the end of the training session.\n",
    "- **I highly recommend saving the network weights if your network once training is done in `fit` (the default). This will save you a lot of time next week when working with the decoder neural networks.** \n",
    "- Training is expected to take ~10-15 mins on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1b6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e28f7a",
   "metadata": {},
   "source": [
    "### 6b. Plot final Competitive Hebbian network weights (MNIST)\n",
    "\n",
    "Use `draw_grid_image` to create a plot of the weights of 100 neurons (e.g. in a 10x10 grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb725db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from viz import draw_grid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266434f",
   "metadata": {},
   "source": [
    "### 6c. Questions\n",
    "\n",
    "**Question 4:** Do the network weights exactly match specific samples in the dataset? Why or why not?\n",
    "\n",
    "**Question 5:** Interpret the meaning of the positive and negative values within the same neuron's weights. How do these factors affect processing? It might be helpful to refer to specific example(s) in your visualization.\n",
    "\n",
    "**Question 6:** Train your network when the competition is \"winner-take-all\" — the winner gets activation 1 and every other neuron gets activation 0. Generate a plot of the weights to showcase what you find. How are they qualitatively different from before? *You shouldn't need to train for long to see the difference.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599b847",
   "metadata": {},
   "source": [
    "**Answer 4:** YOUR ANSWER HERE\n",
    "\n",
    "**Answer 5:** YOUR ANSWER HERE\n",
    "\n",
    "**Answer 6:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159d8c4",
   "metadata": {},
   "source": [
    "### 6d. Train the Hebbian network on CIFAR-10\n",
    "\n",
    "Repeat what you did above, but this time for CIFAR-10.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "I suggest using the same hyperparameters that you used for MNIST, except for the following:\n",
    "\n",
    "- 250 epochs\n",
    "- $k = 2$ neurons that achieve the 2nd highest netIn should be inhibited to $-\\Delta$\n",
    "- `5e-4` learning rate\n",
    "\n",
    "**Notes:**\n",
    "- Use the centered version of CIFAR-10, as you did for MNIST.\n",
    "- The training time on the GPU should we similar to that required by MNIST.\n",
    "- The weight visualization is neat and should be quite different from what you observed MNIST.\n",
    "- **Use a different name for the weights that get exported. You do NOT want to overwrite your \"good\" MNIST weights!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06078a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198e9ed",
   "metadata": {},
   "source": [
    "### 6e. Questions\n",
    "\n",
    "**Question 7:** Make a plot showing the final weights of 100 neurons in a 10x10 grid. What do you glean about what the neurons learn to encode about the CIFAR-10 dataset? \n",
    "\n",
    "**Question 8:** To what extent are characteristics of the encoding similar or different to what you obtained with MNIST?\n",
    "\n",
    "**Question 9:** Copy your code to train the competitive Hebbian network on CIFAR-10 below and make the following changes:\n",
    "- Turn off the live visualization.\n",
    "- When you call `fit`, set `save_wts` to `False` (**do NOT overwrite your \"good\" CIFAR-10 weights!**).\n",
    "- Lower the number of epochs to something in the range 5-15.\n",
    "\n",
    "Use the `time` module to time how long the training session takes both on the CPU and on the GPU (two different training sessions). Which is faster and by how large of a margin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a0667",
   "metadata": {},
   "source": [
    "**Answer 7:** YOUR ANSWER HERE\n",
    "\n",
    "**Answer 8:** YOUR ANSWER HERE\n",
    "\n",
    "**Answer 9:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafeffc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c7917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffdd19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs443",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
